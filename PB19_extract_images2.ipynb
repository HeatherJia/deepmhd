{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import *\n",
    "from astropy.io import fits\n",
    "from skimage import exposure\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except ImportError:  # python 3.x\n",
    "    import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define simualtion data names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "download = False\n",
    "if download:\n",
    "    !wget -O dens.b1_p01_n1.fits https://faun.rc.fas.harvard.edu/bburkhart/b1p.01_512/dens_t500.fits.gz --no-check-certificate\n",
    "    !wget -O dens.b1_p01_n2.fits https://faun.rc.fas.harvard.edu/bburkhart/b1p.01_512/dens_t550.fits.gz --no-check-certificate\n",
    "    !wget -O dens.b1_p01_n3.fits https://faun.rc.fas.harvard.edu/bburkhart/b1p.01_512/dens_t600.fits.gz --no-check-certificate\n",
    "    !wget -O dens.b1_p01_n4.fits https://faun.rc.fas.harvard.edu/bburkhart/b1p.01_512/dens_t650.fits.gz --no-check-certificate\n",
    "    !wget -O dens.b1_p01_n5.fits https://faun.rc.fas.harvard.edu/bburkhart/b1p.01_512/dens_t700.fits.gz --no-check-certificate\n",
    "    \n",
    "    !wget -O dens.b01_p01_n1.fits https://faun.rc.fas.harvard.edu/bburkhart/b.1p.01_512/dens_t500.fits.gz --no-check-certificate\n",
    "    !wget -O dens.b01_p01_n2.fits https://faun.rc.fas.harvard.edu/bburkhart/b.1p.01_512/dens_t550.fits.gz --no-check-certificate\n",
    "    !wget -O dens.b01_p01_n3.fits https://faun.rc.fas.harvard.edu/bburkhart/b.1p.01_512/dens_t600.fits.gz --no-check-certificate\n",
    "    !wget -O dens.b01_p01_n4.fits https://faun.rc.fas.harvard.edu/bburkhart/b.1p.01_512/dens_t650.fits.gz --no-check-certificate\n",
    "    !wget -O dens.b01_p01_n5.fits https://faun.rc.fas.harvard.edu/bburkhart/b.1p.01_512/dens_t700.fits.gz --no-check-certificate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### note that we do not use n4, the 6.5 eddy turnover time snapshot\n",
    "\n",
    "# super-alfvénic\n",
    "sub_alfvens = ['dens.b1_p01_n1.fits', 'dens.b1_p01_n2.fits', \n",
    "               'dens.b1_p01_n3.fits', 'dens.b1_p01_n4.fits', 'dens.b1_p01_n5.fits']\n",
    "\n",
    "# sub-alfvénic\n",
    "sup_alfvens = ['dens.b01_p01_n1.fits', 'dens.b01_p01_n2.fits', \n",
    "               'dens.b01_p01_n3.fits', 'dens.b01_p01_n4.fits', 'dens.b01_p01_n5.fits']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FFP = True\n",
    "ffpname = ''\n",
    "if FFP:\n",
    "    ffpname='.ffp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# code to extract the images\n",
    "def makeimages(data, imsize, nx, sh, nsh, idstem, filepath, saveimagesindividually=False, killpwr=False):\n",
    "    imsize = int(imsize)\n",
    "    dsh = data.shape\n",
    "    newdata = data\n",
    "    cube = np.zeros([imsize, imsize, int(dsh[2]*nsh*nsh*nx*nx)])\n",
    "    iterct = 0\n",
    "    IDs = []\n",
    "    for s in np.arange(dsh[2]):\n",
    "        if killpwr:\n",
    "            img = data[:, :, s]\n",
    "            fftsl= np.fft.fft2(img)\n",
    "            normalized = fftsl/np.abs(fftsl)\n",
    "            newimg = np.real(np.fft.ifft2(normalized))\n",
    "            newdata[:, :, s] = newimg\n",
    "    for s in tqdm(np.arange(dsh[2])):\n",
    "        for r in np.arange(nsh):\n",
    "            for q in np.arange(nsh):\n",
    "                fullimg = np.roll(np.roll(newdata[:, :, s], int(r*sh+s), axis=0), int(q*sh+s), axis=1)\n",
    "                for i in np.arange(nx):\n",
    "                    for j in np.arange(nx):\n",
    "                        toimg = fullimg[int(i*imsize):int((i+1)*imsize), int(j*imsize):int((j+1)*imsize)]\n",
    "                        exeq = np.reshape(exposure.equalize_hist(toimg), (imsize, imsize, 1))\n",
    "                        cube[:, :, iterct] = exeq[:, :, 0]\n",
    "                        imgid = idstem+str(iterct)\n",
    "                        IDs.append(imgid)\n",
    "                        if saveimagesindividually:\n",
    "                            np.save(filepath +'/' + imgid+'.npy', exeq)\n",
    "                        iterct = iterct +1\n",
    "                        #exeq = exeq.reshape([imsize, imsize, 1])*np.ones([imsize, imsize, 3])*255\n",
    "                        #img = Image.fromarray(exeq.astype('int8'), mode='RGB')\n",
    "    return cube, IDs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some basic parameters for image extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The size of the images to make\n",
    "imsize = 128\n",
    "# The \"striding\"\n",
    "nsh = 2\n",
    "# The stride size\n",
    "sh = imsize/nsh\n",
    "# The number of images across a cube\n",
    "nx = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the images for the sub-Alfvénic simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_ids = []\n",
    "test_ids = []\n",
    "train_labels = []\n",
    "test_labels = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [01:20<00:00,  6.46it/s]\n",
      "100%|██████████| 512/512 [01:18<00:00,  6.64it/s]\n",
      "100%|██████████| 512/512 [01:19<00:00,  6.73it/s]\n"
     ]
    }
   ],
   "source": [
    "#Make the trainig data for sub-alfvenic case\n",
    "for fn in sub_alfvens[0:3]:\n",
    "    cube = fits.getdata(fn)\n",
    "    data = np.log10(cube)\n",
    "    data = (data- np.nanmin(data))/(np.nanmax(data)- np.nanmin(data))*255.0\n",
    "    shcube = data.shape\n",
    "    data[~np.isfinite(data)] = 0.\n",
    "\n",
    "    train_x, ids= makeimages(data, int(imsize), nx, sh, nsh, 'subalf1_train'+ffpname, 'data', saveimagesindividually=True, killpwr=FFP)\n",
    "    train_ids=train_ids+ids\n",
    "    train_labels = train_labels+list(np.zeros(len(ids), dtype='int'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the images for the super-Alfvénic simulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [01:21<00:00,  6.64it/s]\n",
      "100%|██████████| 512/512 [01:19<00:00,  6.63it/s]\n",
      "100%|██████████| 512/512 [01:19<00:00,  6.55it/s]\n"
     ]
    }
   ],
   "source": [
    "#Make the training data for super-alfvenic\n",
    "for fn in sup_alfvens[0:3]:\n",
    "    cube = fits.getdata(fn)\n",
    "    data = np.log10(cube)\n",
    "    data = (data- np.nanmin(data))/(np.nanmax(data)- np.nanmin(data))*255.0\n",
    "    shcube = data.shape\n",
    "    data[~np.isfinite(data)] = 0.\n",
    "\n",
    "    train_x, ids= makeimages(data, int(imsize), nx, sh, nsh, 'supalf1_train'+ffpname, 'data', saveimagesindividually=True, killpwr=FFP)\n",
    "    train_ids=train_ids+ids\n",
    "    train_labels = train_labels+list(np.ones(len(ids), dtype='int'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [01:21<00:00,  7.09it/s]\n"
     ]
    }
   ],
   "source": [
    "#Make the test data for sub-alfvenic case\n",
    "for fn in sub_alfvens[4:]:\n",
    "    cube = fits.getdata(fn)\n",
    "    data = np.log10(cube)\n",
    "    data = (data- np.nanmin(data))/(np.nanmax(data)- np.nanmin(data))*255.0\n",
    "    shcube = data.shape\n",
    "    data[~np.isfinite(data)] = 0.\n",
    "\n",
    "    test_x, ids= makeimages(data, int(imsize), nx, sh, nsh, 'subalf1_test'+ffpname, 'data', saveimagesindividually=True, killpwr=FFP)\n",
    "    test_ids=test_ids+ids\n",
    "    test_labels = test_labels+list(np.zeros(len(ids), dtype='int'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 512/512 [01:18<00:00,  6.69it/s]\n"
     ]
    }
   ],
   "source": [
    "#Make the test data for super-alfvenic\n",
    "for fn in sup_alfvens[4:]:\n",
    "    cube = fits.getdata(fn)\n",
    "    data = np.log10(cube)\n",
    "    data = (data- np.nanmin(data))/(np.nanmax(data)- np.nanmin(data))*255.0\n",
    "    shcube = data.shape\n",
    "    data[~np.isfinite(data)] = 0.\n",
    "\n",
    "    test_x, ids= makeimages(data, int(imsize), nx, sh, nsh, 'supalf1_test'+ffpname, 'data', saveimagesindividually=True, killpwr=FFP)\n",
    "    test_ids=test_ids+ids\n",
    "    test_labels = test_labels+list(np.ones(len(ids), dtype='int'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_labels = train_labels+test_labels\n",
    "all_ids = train_ids + test_ids\n",
    "zip_label_ids = zip(all_ids, all_labels)\n",
    "labels = dict(zip_label_ids)\n",
    "partition = {'train': train_ids, 'validation': test_ids}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('labels5_5.p', 'wb') as fp:\n",
    "    pickle.dump(labels, fp, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "with open('partition5_5.p', 'wb') as fp:\n",
    "    pickle.dump(partition, fp, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [anaconda3]",
   "language": "python",
   "name": "Python [anaconda3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
